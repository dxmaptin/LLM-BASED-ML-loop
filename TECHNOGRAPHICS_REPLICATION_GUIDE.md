# TECHNOGRAPHICS REPLICATION GUIDE
## Complete Step-by-Step Process to Replicate ACORN Training on Technographics Data

**Date:** 2025-11-21
**Purpose:** Replicate the exact ACORN training methodology (V1 → V10) on Technographics segments
**Data Source:** OneDrive_2025-11-21.zip + Technographics - Segment Descriptions.txt

---

## Table of Contents
1. [Data Structure Comparison](#data-structure-comparison)
2. [Phase 1: Data Preparation](#phase-1-data-preparation)
3. [Phase 2: Initial Setup](#phase-2-initial-setup)
4. [Phase 3: Baseline Run (V1)](#phase-3-baseline-run-v1)
5. [Phase 4: Training Loop (V1 → V10)](#phase-4-training-loop-v1--v10)
6. [Phase 5: Validation & Results](#phase-5-validation--results)

---

## Data Structure Comparison

### ACORN (Reference Structure)

```
demographic_runs_ACORN/
├── exclusive_addresses/                    # One directory per demographic
│   ├── Flattened Data Inputs/
│   │   └── ACORN_exclusive_addresses.csv   # Format: Category,Question,Answer,Value
│   ├── Textual Data Inputs/
│   │   └── exclusive_addresses_profile.txt # Demographic description
│   ├── concepts_to_test.csv                # Format: Concept (one per line)
│   ├── context_summary_generated.txt       # Auto-generated by IR Agent
│   └── estimator_results_ACORN.txt         # Auto-generated by Estimator
├── flourishing_capital/
├── ... (22 total demographics)
```

**Ground Truth:**
```
ACORN_ground_truth_named.csv
Format: Class,Question1,Question2,...Question10
```

---

### Technographics (Target Structure)

```
demographic_runs_TECHNO/
├── empowered_pioneers/                     # One directory per segment
│   ├── Flattened Data Inputs/
│   │   └── TECHNO_empowered_pioneers.csv   # Format: Category,Question,Answer,Value
│   ├── Textual Data Inputs/
│   │   └── empowered_pioneers_profile.txt  # From Technographics descriptions
│   ├── concepts_to_test.csv                # Format: Concept (one per line)
│   ├── context_summary_generated.txt       # Auto-generated by IR Agent
│   └── estimator_results_TECHNO.txt        # Auto-generated by Estimator
├── savvy_skeptics/
├── social_explorers/
├── pragmatic_protectionists/
├── relaxed_simplifiers/
├── avoidant_security_seekers/
```

**Ground Truth:**
```
TECHNO_ground_truth.csv
Format: Segment,Question1,Question2,...QuestionN
```

---

## Phase 1: Data Preparation

### Step 1.1: Create Conversion Script

**File:** `convert_technographics_data.py`

```python
#!/usr/bin/env python3
"""
Convert Technographics Excel data to ACORN-compatible format
"""
import pandas as pd
from pathlib import Path
import shutil

# Segment name mappings
SEGMENT_MAPPING = {
    "A+ Empowered Pioneers": "empowered_pioneers",
    "A- Savvy Skeptics": "savvy_skeptics",
    "B+ Social Explorers": "social_explorers",
    "B- Pragmatic Protectionists": "pragmatic_protectionists",
    "C+ Relaxed Simplifiers": "relaxed_simplifiers",
    "C- Avoidant Security Seekers": "avoidant_security_seekers"
}

# Excel files to process
EXCEL_FILES = [
    "OneDrive_extracted/Segment Data Tables/Food - Segment Data Tables.xlsx",
    "OneDrive_extracted/Segment Data Tables/Retail - Segment Data Tables.xlsx",
    "OneDrive_extracted/Segment Data Tables/Travel - Segment Data Tables.xlsx",
    "OneDrive_extracted/Segment Data Tables/Segmentation - Segment Data Tables.xlsx"
]

BASE_DIR = Path("demographic_runs_TECHNO")

def convert_excel_to_flattened_csv(excel_path, segment_name, output_dir):
    """Convert Excel data for one segment to flattened CSV format"""
    df = pd.read_excel(excel_path)

    # Get the column name for this segment
    segment_col = [col for col in df.columns if segment_name.startswith(col.split()[0])][0]

    flattened_rows = []
    for idx, row in df.iterrows():
        question = row["Question"]
        answer = row["Answer code"]
        value = row[segment_col]

        # Determine category from question text
        if "S1" in question or "INDUSTRY" in question:
            category = "Industry"
        elif "Age" in question or "S2" in question:
            category = "Demographics"
        elif "GENDER" in question or "S3" in question:
            category = "Demographics"
        elif "LOCATION" in question or "S4" in question:
            category = "Geography"
        else:
            # Infer from domain
            if "food" in str(excel_path).lower():
                category = "Food Behaviors"
            elif "retail" in str(excel_path).lower():
                category = "Retail Behaviors"
            elif "travel" in str(excel_path).lower():
                category = "Travel Behaviors"
            else:
                category = "Attitudes"

        flattened_rows.append({
            "Category": category,
            "Question": question,
            "Answer": answer,
            "Value": value
        })

    # Save to CSV
    output_df = pd.DataFrame(flattened_rows)
    output_df.to_csv(output_dir / "flattened_data.csv", index=False)

    return output_df


def create_concepts_to_test(segment_dir):
    """
    Create concepts_to_test.csv with 10-15 diverse questions

    Strategy:
    - Select questions that have clear agree/disagree scale
    - Pick from different domains (food, retail, travel, attitudes)
    - Avoid demographic questions (age, gender, location)
    """

    # Read all Excel files to find suitable questions
    all_questions = set()

    for excel_path in EXCEL_FILES:
        df = pd.read_excel(excel_path)

        # Filter for Likert-scale questions
        # Look for questions with answers like "Strongly agree", "Agree", etc.
        for question in df["Question"].unique():
            question_df = df[df["Question"] == question]
            answers = set(question_df["Answer code"].str.lower())

            # Check if it's a Likert-scale question
            likert_keywords = {"agree", "disagree", "likely", "important", "satisfied"}
            if any(keyword in str(answers) for keyword in likert_keywords):
                all_questions.add(question)

    # Select 10-15 diverse questions
    selected_questions = list(all_questions)[:15]

    # Save to concepts_to_test.csv
    concepts_df = pd.DataFrame({"Concept": selected_questions})
    concepts_df.to_csv(segment_dir / "concepts_to_test.csv", index=False)

    return selected_questions


def create_textual_profile(segment_name, segment_dir):
    """Copy segment description from Technographics file"""

    # Read technographics descriptions
    with open("Technographics - Segment Descriptions.txt", "r") as f:
        content = f.read()

    # Extract description for this segment
    segment_display_name = [k for k, v in SEGMENT_MAPPING.items() if v == segment_name][0]

    lines = content.split("\n")
    description = ""
    found = False

    for line in lines:
        if segment_display_name in line:
            found = True
            continue
        if found:
            if line.strip() and not line.startswith(("A+", "A-", "B+", "B-", "C+", "C-")):
                description = line.strip()
                break

    # Save to profile.txt
    textual_dir = segment_dir / "Textual Data Inputs"
    textual_dir.mkdir(parents=True, exist_ok=True)

    with open(textual_dir / f"{segment_name}_profile.txt", "w") as f:
        f.write(f"{segment_display_name}\n\n")
        f.write(f"{description}\n")

    return description


def create_ground_truth(concepts, output_path="TECHNO_ground_truth.csv"):
    """
    Create ground truth CSV with actual values for selected concepts
    Format: Segment,Question1,Question2,...QuestionN
    """

    ground_truth = {}

    for segment_display, segment_name in SEGMENT_MAPPING.items():
        row_data = {"Segment": segment_name}

        for concept in concepts:
            # Find the concept in Excel files
            value = None

            for excel_path in EXCEL_FILES:
                df = pd.read_excel(excel_path)
                concept_df = df[df["Question"] == concept]

                if len(concept_df) > 0:
                    # Get the value for this segment
                    # For Likert questions, sum up "Agree" + "Strongly Agree"
                    segment_data = concept_df[segment_display]

                    # Check if this is Likert-scale
                    answers = concept_df["Answer code"].str.lower()
                    if any("agree" in str(a) for a in answers):
                        # Sum positive responses
                        agree_mask = answers.str.contains("agree", case=False, na=False)
                        value = segment_data[agree_mask].sum()
                    else:
                        # For non-Likert, take mean or most common
                        value = segment_data.mean()

                    break

            row_data[concept] = value if value is not None else 0.0

        ground_truth[segment_name] = row_data

    # Save to CSV
    gt_df = pd.DataFrame(ground_truth.values())
    gt_df.to_csv(output_path, index=False)

    return gt_df


def main():
    """Main conversion pipeline"""

    print("="*80)
    print("TECHNOGRAPHICS DATA CONVERSION")
    print("="*80)

    # Create base directory
    BASE_DIR.mkdir(exist_ok=True)

    all_concepts = set()

    # Step 1: Convert each segment
    for segment_display, segment_name in SEGMENT_MAPPING.items():
        print(f"\n[1/6] Processing: {segment_display} ({segment_name})")

        segment_dir = BASE_DIR / segment_name
        segment_dir.mkdir(exist_ok=True)

        # Create Flattened Data Inputs directory
        flattened_dir = segment_dir / "Flattened Data Inputs"
        flattened_dir.mkdir(exist_ok=True)

        # Convert each Excel file
        all_data = []
        for excel_path in EXCEL_FILES:
            print(f"  - Converting: {Path(excel_path).name}")
            df = convert_excel_to_flattened_csv(excel_path, segment_display, segment_dir)
            all_data.append(df)

        # Merge all data into one CSV
        merged_df = pd.concat(all_data, ignore_index=True)
        merged_df.to_csv(flattened_dir / f"TECHNO_{segment_name}.csv", index=False)

        # Create textual profile
        print(f"  - Creating textual profile")
        create_textual_profile(segment_name, segment_dir)

        # Create concepts_to_test.csv (same for all segments initially)
        print(f"  - Creating concepts_to_test.csv")
        concepts = create_concepts_to_test(segment_dir)
        all_concepts.update(concepts)

    # Step 2: Create ground truth
    print(f"\n[2/6] Creating ground truth CSV")
    create_ground_truth(list(all_concepts))

    print("\n" + "="*80)
    print("CONVERSION COMPLETE!")
    print("="*80)
    print(f"\nOutput directory: {BASE_DIR}")
    print(f"Ground truth: TECHNO_ground_truth.csv")
    print(f"Total segments: {len(SEGMENT_MAPPING)}")
    print(f"Total concepts: {len(all_concepts)}")
    print("\nNext steps:")
    print("1. Review concepts_to_test.csv in each segment directory")
    print("2. Run: python run_all_technographics.py")


if __name__ == "__main__":
    main()
```

### Step 1.2: Run Conversion

```bash
# Run the conversion script
python convert_technographics_data.py
```

**Expected Output:**
```
demographic_runs_TECHNO/
├── empowered_pioneers/
│   ├── Flattened Data Inputs/TECHNO_empowered_pioneers.csv
│   ├── Textual Data Inputs/empowered_pioneers_profile.txt
│   └── concepts_to_test.csv
├── ... (6 total segments)

TECHNO_ground_truth.csv
```

### Step 1.3: Manual Review & Curation

**Review:**
1. Open each `concepts_to_test.csv`
2. Verify questions are Likert-scale attitudes/behaviors
3. Remove demographic questions (age, gender, location)
4. Ensure diversity across domains (food, retail, travel, attitudes)

**Target:** 10-15 high-quality questions per segment

---

## Phase 2: Initial Setup

### Step 2.1: Create Technographics Run Script

**File:** `run_all_technographics.py`

```python
#!/usr/bin/env python3
"""Run full pipeline for all 6 Technographics segments and calculate R²."""

import subprocess
import sys
from pathlib import Path
import time

# All 6 Technographics segments
TECHNO_SEGMENTS = [
    "empowered_pioneers",
    "savvy_skeptics",
    "social_explorers",
    "pragmatic_protectionists",
    "relaxed_simplifiers",
    "avoidant_security_seekers"
]

BASE_DIR = Path("demographic_runs_TECHNO")
PYTHON = Path("venv/Scripts/python.exe")

print("="*80)
print("RUNNING FULL PIPELINE FOR ALL 6 TECHNOGRAPHICS SEGMENTS")
print("="*80)
print(f"\nTotal segments: {len(TECHNO_SEGMENTS)}")
print(f"Pipeline steps per segment:")
print("  1. IR Agent (Data Extraction)")
print("  2. Estimator Agent (Predictions)")
print("  3. Critic Agent (Validation)")
print("\nEstimated time: ~30-40 minutes for all 6 segments")
print("="*80)

results = []
failed_segments = []

for idx, segment_name in enumerate(TECHNO_SEGMENTS, 1):
    print(f"\n[{idx}/6] Processing: {segment_name}")
    print("-" * 80)

    segment_dir = BASE_DIR / segment_name
    context_file = segment_dir / "context_summary_generated.txt"
    estimator_output = segment_dir / "estimator_results_TECHNO.txt"

    # Check if segment directory exists
    if not segment_dir.exists():
        print(f"  ❌ ERROR: Directory not found: {segment_dir}")
        failed_segments.append((segment_name, "Directory not found"))
        continue

    # Step 1: Run IR Agent (Data Extraction)
    print(f"  [1/2] Running IR Agent...")
    start_time = time.time()

    try:
        result = subprocess.run(
            [str(PYTHON), "run_parser_for_dir.py", "--base-dir", str(segment_dir)],
            capture_output=True,
            text=True,
            timeout=300  # 5 minutes timeout
        )

        if result.returncode != 0:
            print(f"  [FAIL] IR Agent failed:")
            print(f"     {result.stderr[:200]}")
            failed_segments.append((segment_name, "IR Agent failed"))
            continue

        if not context_file.exists():
            print(f"  [FAIL] Context file not generated: {context_file}")
            failed_segments.append((segment_name, "Context file not created"))
            continue

        ir_time = time.time() - start_time
        print(f"  [OK] IR Agent completed ({ir_time:.1f}s)")

    except subprocess.TimeoutExpired:
        print(f"  [FAIL] IR Agent timeout (>5 minutes)")
        failed_segments.append((segment_name, "IR Agent timeout"))
        continue
    except Exception as e:
        print(f"  [FAIL] IR Agent error: {e}")
        failed_segments.append((segment_name, f"IR Agent error: {e}"))
        continue

    # Step 2: Run Estimator + Critic
    print(f"  [2/2] Running Estimator + Critic...")
    start_time = time.time()

    try:
        result = subprocess.run(
            [
                str(PYTHON), "run_estimator_from_context.py",
                "--context", str(context_file),
                "--output", str(estimator_output)
            ],
            capture_output=True,
            text=True,
            timeout=600  # 10 minutes timeout
        )

        if result.returncode != 0:
            print(f"  [FAIL] Estimator failed:")
            print(f"     {result.stderr[:200]}")
            failed_segments.append((segment_name, "Estimator failed"))
            continue

        if not estimator_output.exists():
            print(f"  [FAIL] Estimator results not generated")
            failed_segments.append((segment_name, "Estimator output not created"))
            continue

        estimator_time = time.time() - start_time
        print(f"  [OK] Estimator + Critic completed ({estimator_time:.1f}s)")

        results.append({
            "segment": segment_name,
            "ir_time": ir_time,
            "estimator_time": estimator_time,
            "total_time": ir_time + estimator_time
        })

    except subprocess.TimeoutExpired:
        print(f"  [FAIL] Estimator timeout (>10 minutes)")
        failed_segments.append((segment_name, "Estimator timeout"))
        continue
    except Exception as e:
        print(f"  [FAIL] Estimator error: {e}")
        failed_segments.append((segment_name, f"Estimator error: {e}"))
        continue

print("\n" + "="*80)
print("PIPELINE EXECUTION COMPLETE")
print("="*80)

print(f"\n[OK] Successfully processed: {len(results)}/{len(TECHNO_SEGMENTS)} segments")
print(f"[FAIL] Failed: {len(failed_segments)} segments")

if failed_segments:
    print("\nFailed segments:")
    for segment_name, reason in failed_segments:
        print(f"  - {segment_name}: {reason}")

if results:
    total_time = sum(r["total_time"] for r in results)
    avg_time = total_time / len(results)
    print(f"\nTotal processing time: {total_time:.1f}s ({total_time/60:.1f} minutes)")
    print(f"Average time per segment: {avg_time:.1f}s")

print("\n" + "="*80)
print("Next step: Run calculate_r2_technographics.py to compute R² for all segments")
print("="*80)
```

### Step 2.2: Create R² Calculation Script

**File:** `calculate_r2_technographics.py`

```python
#!/usr/bin/env python3
"""Calculate R² scores for Technographics segments"""

import pandas as pd
from pathlib import Path
from sklearn.metrics import r2_score, mean_absolute_error
import numpy as np

BASE_DIR = Path("demographic_runs_TECHNO")
GROUND_TRUTH_FILE = "TECHNO_ground_truth.csv"

SEGMENTS = [
    "empowered_pioneers",
    "savvy_skeptics",
    "social_explorers",
    "pragmatic_protectionists",
    "relaxed_simplifiers",
    "avoidant_security_seekers"
]

def calculate_r2_for_segment(segment_name, ground_truth_df):
    """Calculate R² for one segment"""

    results_file = BASE_DIR / segment_name / "estimator_results_TECHNO.txt"

    if not results_file.exists():
        print(f"  ❌ Results file not found: {results_file}")
        return None

    # Read predictions
    with open(results_file, "r") as f:
        predictions_text = f.read()

    # Parse predictions (format depends on estimator output)
    # This is a placeholder - adjust based on actual format
    predictions = {}

    # Extract predictions for each concept
    # ... parsing logic here ...

    # Get ground truth for this segment
    gt_row = ground_truth_df[ground_truth_df["Segment"] == segment_name]

    if len(gt_row) == 0:
        print(f"  ❌ No ground truth found for {segment_name}")
        return None

    # Calculate R²
    y_true = []
    y_pred = []

    for concept in gt_row.columns:
        if concept == "Segment":
            continue

        if concept in predictions:
            y_true.append(gt_row[concept].iloc[0])
            y_pred.append(predictions[concept])

    if len(y_true) < 3:
        print(f"  ⚠️  Not enough data points for {segment_name}")
        return None

    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)

    return {
        "segment": segment_name,
        "r2": r2,
        "mae": mae,
        "n_concepts": len(y_true)
    }


def main():
    print("="*80)
    print("CALCULATING R² FOR TECHNOGRAPHICS SEGMENTS")
    print("="*80)

    # Load ground truth
    ground_truth_df = pd.read_csv(GROUND_TRUTH_FILE)

    results = []

    for segment in SEGMENTS:
        print(f"\nCalculating R² for: {segment}")
        result = calculate_r2_for_segment(segment, ground_truth_df)

        if result:
            print(f"  R² = {result['r2']:.4f}")
            print(f"  MAE = {result['mae']*100:.2f}pp")
            results.append(result)

    # Save results
    results_df = pd.DataFrame(results)
    results_df.to_csv("technographics_r2_results.csv", index=False)

    print("\n" + "="*80)
    print("RESULTS SUMMARY")
    print("="*80)
    print(f"\nAverage R²: {results_df['r2'].mean():.4f}")
    print(f"Average MAE: {results_df['mae'].mean()*100:.2f}pp")
    print(f"\nResults saved to: technographics_r2_results.csv")


if __name__ == "__main__":
    main()
```

---

## Phase 3: Baseline Run (V1)

### Step 3.1: Initial Prompt Setup

**File:** `agent_estimator/estimator_agent/prompts/technographics_prompts.py`

```python
"""Technographics-specific prompts (V1 baseline)"""

# V1: Start with generic ACORN prompt, no segment-specific calibrations

TECHNO_GENERAL_PROMPT = """
# Technographic Segment Prediction Task

You are predicting consumer attitudes and behaviors for technology-based segments.

## Decision Workflow

1. Identify the technographic segment archetype
2. Classify concept type (attitude/behavior/identity)
3. Apply baseline distribution
4. Review evidence and adjust
5. Apply segment calibrations (if any)
6. Sanity check predictions
7. Generate final distribution

## Concept-Type Baselines

**Attitude Statement** (e.g., "I think brands should...")
- Baseline: SA 12%, A 25%, N 32%, SD 20%, SDD 11%

**Behavior - Low Friction** (e.g., "use", "prefer")
- Baseline: SA 18%, A 27%, N 28%, SD 17%, SDD 10%

**Behavior - High Friction** (e.g., "switch", "invest")
- Baseline: SA 8%, A 18%, N 38%, SD 22%, SDD 14%

**Identity Statement** (e.g., "I am...")
- Baseline: SA 10%, A 22%, N 42%, SD 18%, SDD 8%

## Evidence Hierarchy

1. **Proximal Evidence** (relevance >0.90): 80% weight
2. **Related Quantitative** (relevance 0.70-0.90): 50% weight
3. **Qualitative Insights** (relevance 0.50-0.70): 20% weight
4. **Segment Inference** (relevance <0.50): 30% weight

## Critical Rules

- DO NOT regress to mean when strong evidence exists
- If proximal ≥80%: Predict 75-95% (be bold!)
- If proximal ≤20%: Predict 10-30% (be bold!)
- Respect evidence quality and relevance

"""

# Segment-specific prompts (V1: Empty, will be populated during training)
TECHNO_SEGMENT_PROMPTS = {
    "empowered_pioneers": "",
    "savvy_skeptics": "",
    "social_explorers": "",
    "pragmatic_protectionists": "",
    "relaxed_simplifiers": "",
    "avoidant_security_seekers": ""
}
```

### Step 3.2: Run Baseline

```bash
# Run V1 baseline
python run_all_technographics.py

# Calculate R²
python calculate_r2_technographics.py
```

**Expected V1 Results:**
- Average R²: 0.50-0.65 (generic baseline)
- High variance across segments
- Likely regression to mean (all predictions 40-60%)

**Save Results:**
```bash
# Version the results
cp technographics_r2_results.csv technographics_r2_results_v1.csv
```

---

## Phase 4: Training Loop (V1 → V10)

This is where you replicate the exact ACORN training methodology.

### Iteration Template

For each iteration (V2, V3, ... V10):

#### **Step 4.1: Error Analysis**

```python
# Analyze V1 results
import pandas as pd

v1_results = pd.read_csv("technographics_r2_results_v1.csv")

# Identify patterns:
# 1. Which segments underperform?
# 2. Which questions have highest errors?
# 3. Any systematic biases (over/under-prediction)?

# Example analysis:
print("Segments by R²:")
print(v1_results.sort_values("r2"))

print("\nSegments with R² < 0.60:")
failing = v1_results[v1_results["r2"] < 0.60]
print(failing)

# For each failing segment, examine predictions vs ground truth
for segment in failing["segment"]:
    print(f"\n{segment}:")
    # ... detailed error analysis ...
```

#### **Step 4.2: Update Prompts**

Based on error analysis, update prompts in `technographics_prompts.py`:

**Example Updates:**

```python
# V2: Add segment-specific calibrations

TECHNO_SEGMENT_PROMPTS = {
    "empowered_pioneers": """
[Profile]
Highly tech-capable, early adopters, high AI trust, confident with digital tools

[Learned Calibrations - V2]
1. DIGITAL BEHAVIORS: LIFT by +25-30pp (highest adoption group)
   - Expect 80-95% for online/app/tech behaviors
   - Proximal ≥70% → predict 85-98%

2. INNOVATION ADOPTION: LIFT by +20pp
   - New products/services: Expect 70-90%
   - AI/automation: Expect 75-95%

3. TRUST: Higher baseline
   - Digital trust: +15pp above baseline
   - Privacy concerns: -20pp below baseline
""",

    "avoidant_security_seekers": """
[Profile]
Low tech usage, high caution, distrust digital tools, prefer familiar channels

[Learned Calibrations - V2]
1. DIGITAL BEHAVIORS: REDUCE by -30-40pp (lowest adoption group)
   - Expect 10-30% for online/app/tech behaviors
   - Proximal ≤30% → predict 5-25%

2. TRUST: Lower baseline
   - Digital trust: -25pp below baseline
   - Privacy concerns: +30pp above baseline
   - Scam awareness: +25pp

3. TRADITIONAL PREFERENCE:
   - In-store shopping: +30pp
   - Cash usage: +25pp
   - Phone/face-to-face: +20pp
""",

    # ... other segments ...
}
```

#### **Step 4.3: Validate Changes**

```bash
# Run with V2 prompts
python run_all_technographics.py

# Calculate new R²
python calculate_r2_technographics.py

# Compare V1 vs V2
python -c "
import pandas as pd
v1 = pd.read_csv('technographics_r2_results_v1.csv')
v2 = pd.read_csv('technographics_r2_results.csv')

merged = v1.merge(v2, on='segment', suffixes=('_v1', '_v2'))
merged['improvement'] = merged['r2_v2'] - merged['r2_v1']

print('V1 → V2 Improvements:')
print(merged[['segment', 'r2_v1', 'r2_v2', 'improvement']])
print(f'\nAverage improvement: {merged["improvement"].mean():.4f}')
"

# Save V2 results
cp technographics_r2_results.csv technographics_r2_results_v2.csv
```

#### **Step 4.4: Iterate**

Repeat Steps 4.1-4.3 for V3, V4, ... until:
- Target R² achieved (e.g., 0.80+)
- Pass rate ≥80% (segments with R² > 0.70)
- Improvements plateau (<0.03 per iteration)

---

### Typical Training Trajectory

Based on ACORN experience:

| Version | Focus | Expected R² | Improvement |
|---------|-------|-------------|-------------|
| **V1** | Generic baseline | 0.55 | Baseline |
| **V2** | Segment calibrations | 0.68 | +0.13 |
| **V3** | Question-specific fixes | 0.75 | +0.07 |
| **V4** | Evidence hierarchy tuning | 0.80 | +0.05 |
| **V5** | Edge case handling | 0.83 | +0.03 |
| **V6-V10** | Fine-tuning | 0.85-0.88 | +0.01-0.02 |

**Expected Timeline:** 2-3 weeks (5-10 iterations)

---

### Key Patterns to Look For

#### **Pattern 1: Tech-Adoption Spectrum**
- **Empowered Pioneers:** Over-predict digital by +25-30pp
- **Avoidant Security Seekers:** Under-predict digital by -30-40pp
- **Middle segments:** ±10-15pp adjustments

#### **Pattern 2: Trust vs Skepticism**
- **Savvy Skeptics:** High capability but LOW trust → nuanced predictions
- Don't conflate tech-skill with tech-trust

#### **Pattern 3: Domain-Specific Behaviors**
- **Food:** May show less tech-differentiation (everyone eats)
- **Retail:** Strong online vs in-store split
- **Travel:** High segment differentiation (pioneers use apps heavily)

---

## Phase 5: Validation & Results

### Step 5.1: Final Comprehensive Test

After reaching V10 (or final version):

```bash
# Run final test on all 6 segments
python run_all_technographics.py

# Calculate final R²
python calculate_r2_technographics.py
```

### Step 5.2: Create Results Summary

**File:** `TECHNOGRAPHICS_FINAL_RESULTS.md`

```markdown
# Technographics Training Results

## Final Performance (V10)

| Segment | R² | MAE | Grade | Pass |
|---------|-----|-----|-------|------|
| Empowered Pioneers | 0.XX | X.XX% | X | ✅/❌ |
| Savvy Skeptics | 0.XX | X.XX% | X | ✅/❌ |
| Social Explorers | 0.XX | X.XX% | X | ✅/❌ |
| Pragmatic Protectionists | 0.XX | X.XX% | X | ✅/❌ |
| Relaxed Simplifiers | 0.XX | X.XX% | X | ✅/❌ |
| Avoidant Security Seekers | 0.XX | X.XX% | X | ✅/❌ |
| **Average** | **0.XX** | **X.XX%** | **X** | **X/6** |

## Training Progress

| Version | Avg R² | Improvement | Key Changes |
|---------|--------|-------------|-------------|
| V1 | 0.XX | Baseline | Generic prompts |
| V2 | 0.XX | +0.XX | Segment calibrations |
| ... | ... | ... | ... |
| V10 | 0.XX | +0.XX | Final tuning |

**Total Improvement:** +0.XX (XX%)

## Comparison: ACORN vs Technographics

| Metric | ACORN (22 segments) | Technographics (6 segments) |
|--------|---------------------|------------------------------|
| Average R² | 0.84 | 0.XX |
| Pass Rate | 90.5% (19/21) | XX% (X/6) |
| Average MAE | 5.78pp | X.XX% |
| Training Time | 8 weeks | X weeks |

## Key Learnings

1. ...
2. ...
3. ...

## Recommendations

1. ...
2. ...
3. ...
```

### Step 5.3: Comparative Analysis

**File:** `compare_acorn_technographics.py`

```python
#!/usr/bin/env python3
"""Compare ACORN and Technographics training results"""

import pandas as pd
import matplotlib.pyplot as plt

# Load results
acorn = pd.read_csv("v10_all_classes_comprehensive_results.csv")
techno = pd.read_csv("technographics_r2_results.csv")

# Compare distributions
print("="*80)
print("ACORN vs TECHNOGRAPHICS COMPARISON")
print("="*80)

print("\nACORN (22 segments):")
print(f"  Average R²: {acorn['R2'].mean():.4f}")
print(f"  Pass Rate: {(acorn['R2'] > 0.70).sum()}/{len(acorn)}")
print(f"  Median R²: {acorn['R2'].median():.4f}")

print("\nTechnographics (6 segments):")
print(f"  Average R²: {techno['r2'].mean():.4f}")
print(f"  Pass Rate: {(techno['r2'] > 0.70).sum()}/{len(techno)}")
print(f"  Median R²: {techno['r2'].median():.4f}")

# Hypothesis test: Are Technographics easier to predict?
from scipy.stats import ttest_ind
t_stat, p_value = ttest_ind(acorn['R2'], techno['r2'])
print(f"\nStatistical Test:")
print(f"  t-statistic: {t_stat:.4f}")
print(f"  p-value: {p_value:.4f}")

if p_value < 0.05:
    if techno['r2'].mean() > acorn['R2'].mean():
        print("  ✅ Technographics significantly EASIER to predict")
    else:
        print("  ⚠️  Technographics significantly HARDER to predict")
else:
    print("  ➡️  No significant difference in difficulty")

# Create comparison plot
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].hist(acorn['R2'], bins=10, alpha=0.7, label='ACORN')
axes[0].set_title('ACORN R² Distribution')
axes[0].set_xlabel('R²')
axes[0].set_ylabel('Frequency')

axes[1].hist(techno['r2'], bins=6, alpha=0.7, label='Technographics', color='orange')
axes[1].set_title('Technographics R² Distribution')
axes[1].set_xlabel('R²')
axes[1].set_ylabel('Frequency')

plt.tight_layout()
plt.savefig('acorn_vs_technographics_comparison.png')
print("\nPlot saved: acorn_vs_technographics_comparison.png")
```

---

## Summary Checklist

### Data Preparation
- [ ] Extract OneDrive_2025-11-21.zip
- [ ] Create `convert_technographics_data.py`
- [ ] Run conversion script
- [ ] Verify directory structure matches ACORN format
- [ ] Review and curate `concepts_to_test.csv` for each segment
- [ ] Validate ground truth CSV

### Initial Setup
- [ ] Create `run_all_technographics.py`
- [ ] Create `calculate_r2_technographics.py`
- [ ] Create `technographics_prompts.py` (V1 baseline)
- [ ] Test pipeline on 1 segment manually

### Training Loop (V1 → V10)
- [ ] **V1:** Run baseline, calculate R²
- [ ] **V2:** Add segment calibrations
- [ ] **V3:** Question-specific fixes
- [ ] **V4:** Evidence hierarchy tuning
- [ ] **V5-V10:** Iterative refinement
- [ ] Document each iteration in git commits

### Validation
- [ ] Final comprehensive test
- [ ] Create results summary
- [ ] Compare ACORN vs Technographics
- [ ] Document key learnings

---

## Expected Outcomes

### Hypothesis: Technographics Should Perform Better

**Reasons:**
1. **Fewer segments** (6 vs 22) → clearer boundaries
2. **Behavioral basis** → more directly observable than socio-economic
3. **Tech-specific** → less universal patterns, more differentiation
4. **Richer data** → Multiple domains (food/retail/travel)

**Predicted Results:**
- Average R²: **0.85-0.90** (vs ACORN 0.84)
- Pass Rate: **100%** (6/6) (vs ACORN 90.5%)
- Training Time: **2-3 weeks** (vs ACORN 8 weeks, due to fewer segments)

### Key Success Metrics

- [ ] Average R² > 0.80
- [ ] Pass Rate ≥ 80% (5/6 segments)
- [ ] MAE < 8pp
- [ ] Training converges in < 10 iterations

---

## Troubleshooting

### Issue: R² not improving after V3
**Solution:** Check for:
- Over-fitting to training questions
- Need for question-specific rules
- Domain-specific patterns (food vs retail vs travel)

### Issue: High variance across segments
**Solution:**
- Review segment descriptions quality
- Check data balance across segments
- Consider segment-specific question selection

### Issue: Predictions regressing to mean
**Solution:**
- Strengthen proximal guardrails
- Add bolder prediction rules
- Review evidence quality from IR Agent

---

## Timeline Estimate

| Phase | Task | Time |
|-------|------|------|
| 1 | Data preparation | 2-3 hours |
| 2 | Initial setup | 1-2 hours |
| 3 | Baseline run (V1) | 1 hour |
| 4 | Training loop (V2-V10) | 2-3 weeks |
| 5 | Validation & analysis | 1-2 days |
| **Total** | | **3-4 weeks** |

---

**Last Updated:** 2025-11-21
**Status:** Ready for execution
**Next Action:** Run Step 1.1 (Data Conversion)
